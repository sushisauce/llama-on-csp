{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "835d9d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q \"transformers==4.31.0\" \n",
    "!pip install -q \"datasets[s3]==2.13.0\" \n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q accelerate\n",
    "!pip install -q peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c39547c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcbfdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl 'https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip' -o 'awscliv2.zip'; unzip awscliv2.zip; ./aws/install -i /usr/local/aws-cli -b /usr/local/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65342b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync 's3://sagemaker-us-east-2-236478807796/processed/llama/dolly/train' '/llama/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "942134cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 776/776 [00:00<00:00, 5.89MB/s]\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 62.8MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 37.9MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 4.20MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-13b-hf\" # sharded weights\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,use_auth_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a537ff22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging into the Hugging Face Hub with token hf_oaWwQlP...\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n",
      "Logging into the Hugging Face Hub with token hf_oaWwQlP...\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Logging into the Hugging Face Hub with token hf_oaWwQlP...\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Logging into the Hugging Face Hub with token hf_oaWwQlP...\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Logging into the Hugging Face Hub with token hf_oaWwQlP...\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Logging into the Hugging Face Hub with token hf_oaWwQlP...\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Logging into the Hugging Face Hub with token hf_oaWwQlP...\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n",
      "Logging into the Hugging Face Hub with token hf_oaWwQlP...\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:04<00:00,  1.55s/it]\n",
      "Found 7 modules to quantize: ['k_proj', 'o_proj', 'down_proj', 'up_proj', 'q_proj', 'v_proj', 'gate_proj']\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:05<00:00,  1.93s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:05<00:00,  1.95s/it]\n",
      "Found 7 modules to quantize: ['down_proj', 'q_proj', 'o_proj', 'up_proj', 'k_proj', 'gate_proj', 'v_proj']\n",
      "Loading checkpoint shards:  67%|████████████      | 2/3 [00:05<00:02,  2.88s/it]Found 7 modules to quantize: ['up_proj', 'k_proj', 'o_proj', 'gate_proj', 'v_proj', 'down_proj', 'q_proj']\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:07<00:00,  2.36s/it]\n",
      "Found 7 modules to quantize: ['v_proj', 'q_proj', 'o_proj', 'k_proj', 'down_proj', 'up_proj', 'gate_proj']\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:07<00:00,  2.47s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:07<00:00,  2.46s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:07<00:00,  2.47s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:07<00:00,  2.47s/it]\n",
      "Found 7 modules to quantize: ['q_proj', 'v_proj', 'k_proj', 'up_proj', 'o_proj', 'down_proj', 'gate_proj']\n",
      "Found 7 modules to quantize: ['down_proj', 'up_proj', 'o_proj', 'k_proj', 'q_proj', 'v_proj', 'gate_proj']\n",
      "Found 7 modules to quantize: ['o_proj', 'up_proj', 'gate_proj', 'v_proj', 'down_proj', 'q_proj', 'k_proj']\n",
      "Found 7 modules to quantize: ['up_proj', 'down_proj', 'o_proj', 'gate_proj', 'q_proj', 'k_proj', 'v_proj']\n",
      "trainable params: 250,347,520 || all params: 6,922,327,040 || trainable%: 3.6165225733108386\n",
      "trainable params: 250,347,520 || all params: 6,922,327,040 || trainable%: 3.6165225733108386\n",
      "trainable params: 250,347,520 || all params: 6,922,327,040 || trainable%: 3.6165225733108386\n",
      "trainable params: 250,347,520 || all params: 6,922,327,040 || trainable%: 3.6165225733108386\n",
      "trainable params: 250,347,520 || all params: 6,922,327,040 || trainable%: 3.6165225733108386\n",
      "trainable params: 250,347,520 || all params: 6,922,327,040 || trainable%: 3.6165225733108386\n",
      "trainable params: 250,347,520 || all params: 6,922,327,040 || trainable%: 3.6165225733108386\n",
      "trainable params: 250,347,520 || all params: 6,922,327,040 || trainable%: 3.6165225733108386\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"/llama/im-a-llama.py\", line 256, in <module>\n",
      "    main()\n",
      "  File \"/llama/im-a-llama.py\", line 252, in main\n",
      "    training_function(args)\n",
      "  File \"/llama/im-a-llama.py\", line 215, in training_function\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1539, in train\n",
      "Traceback (most recent call last):\n",
      "    return inner_training_loop(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1656, in _inner_training_loop\n",
      "  File \"/llama/im-a-llama.py\", line 256, in <module>\n",
      "    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 1201, in prepare\n",
      "    main()\n",
      "  File \"/llama/im-a-llama.py\", line 252, in main\n",
      "    training_function(args)\n",
      "  File \"/llama/im-a-llama.py\", line 215, in training_function\n",
      "    raise ValueError(\n",
      "ValueError: You can't train a model that has been loaded with `device_map='auto'` in any distributed mode. Please rerun your script specifying `--num_processes=1` or by launching with `python {{myscript.py}}`.\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1539, in train\n",
      "Traceback (most recent call last):\n",
      "  File \"/llama/im-a-llama.py\", line 256, in <module>\n",
      "    return inner_training_loop(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1656, in _inner_training_loop\n",
      "    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)    \n",
      "main()  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 1201, in prepare\n",
      "\n",
      "  File \"/llama/im-a-llama.py\", line 252, in main\n",
      "    training_function(args)\n",
      "  File \"/llama/im-a-llama.py\", line 215, in training_function\n",
      "    raise ValueError(\n",
      "ValueError: You can't train a model that has been loaded with `device_map='auto'` in any distributed mode. Please rerun your script specifying `--num_processes=1` or by launching with `python {{myscript.py}}`.\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1539, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1656, in _inner_training_loop\n",
      "    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 1201, in prepare\n",
      "    raise ValueError(\n",
      "ValueError: You can't train a model that has been loaded with `device_map='auto'` in any distributed mode. Please rerun your script specifying `--num_processes=1` or by launching with `python {{myscript.py}}`.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"/llama/im-a-llama.py\", line 256, in <module>\n",
      "    main()\n",
      "  File \"/llama/im-a-llama.py\", line 252, in main\n",
      "    training_function(args)\n",
      "  File \"/llama/im-a-llama.py\", line 215, in training_function\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1539, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1656, in _inner_training_loop\n",
      "    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 1201, in prepare\n",
      "    raise ValueError(\n",
      "ValueError: You can't train a model that has been loaded with `device_map='auto'` in any distributed mode. Please rerun your script specifying `--num_processes=1` or by launching with `python {{myscript.py}}`.\n",
      "Traceback (most recent call last):\n",
      "  File \"/llama/im-a-llama.py\", line 256, in <module>\n",
      "    main()\n",
      "  File \"/llama/im-a-llama.py\", line 252, in main\n",
      "    training_function(args)\n",
      "  File \"/llama/im-a-llama.py\", line 215, in training_function\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1539, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1656, in _inner_training_loop\n",
      "Traceback (most recent call last):\n",
      "  File \"/llama/im-a-llama.py\", line 256, in <module>\n",
      "    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 1201, in prepare\n",
      "    raise ValueError(\n",
      "ValueError: You can't train a model that has been loaded with `device_map='auto'` in any distributed mode. Please rerun your script specifying `--num_processes=1` or by launching with `python {{myscript.py}}`.\n",
      "    main()\n",
      "  File \"/llama/im-a-llama.py\", line 252, in main\n",
      "    training_function(args)\n",
      "  File \"/llama/im-a-llama.py\", line 215, in training_function\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1539, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1656, in _inner_training_loop\n",
      "    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 1201, in prepare\n",
      "    raise ValueError(\n",
      "ValueError: You can't train a model that has been loaded with `device_map='auto'` in any distributed mode. Please rerun your script specifying `--num_processes=1` or by launching with `python {{myscript.py}}`.\n",
      "Traceback (most recent call last):\n",
      "  File \"/llama/im-a-llama.py\", line 256, in <module>\n",
      "    main()\n",
      "  File \"/llama/im-a-llama.py\", line 252, in main\n",
      "    training_function(args)\n",
      "  File \"/llama/im-a-llama.py\", line 215, in training_function\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1539, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1656, in _inner_training_loop\n",
      "    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 1201, in prepare\n",
      "    raise ValueError(\n",
      "ValueError: You can't train a model that has been loaded with `device_map='auto'` in any distributed mode. Please rerun your script specifying `--num_processes=1` or by launching with `python {{myscript.py}}`.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"/llama/im-a-llama.py\", line 256, in <module>\n",
      "    main()\n",
      "  File \"/llama/im-a-llama.py\", line 252, in main\n",
      "    training_function(args)\n",
      "  File \"/llama/im-a-llama.py\", line 215, in training_function\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1539, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1656, in _inner_training_loop\n",
      "    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 1201, in prepare\n",
      "    raise ValueError(\n",
      "ValueError: You can't train a model that has been loaded with `device_map='auto'` in any distributed mode. Please rerun your script specifying `--num_processes=1` or by launching with `python {{myscript.py}}`.\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 60958 closing signal SIGTERM\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 60959 closing signal SIGTERM\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 60960 closing signal SIGTERM\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 60962 closing signal SIGTERM\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 60963 closing signal SIGTERM\n",
      "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 60956) of binary: /usr/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\n",
      "    args.func(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 977, in launch_command\n",
      "    multi_gpu_launcher(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 646, in multi_gpu_launcher\n",
      "    distrib_run.run(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 785, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "im-a-llama.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "[1]:\n",
      "  time      : 2023-08-16_00:48:10\n",
      "  host      : 5006739\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 60957)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[2]:\n",
      "  time      : 2023-08-16_00:48:10\n",
      "  host      : 5006739\n",
      "  rank      : 5 (local_rank: 5)\n",
      "  exitcode  : 1 (pid: 60961)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2023-08-16_00:48:10\n",
      "  host      : 5006739\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 60956)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch im-a-llama.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c6981bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e62ff1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration already exists at /root/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n"
     ]
    }
   ],
   "source": [
    "!python -c \"from accelerate.utils import write_basic_config; write_basic_config(mixed_precision='bf16')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3cc81345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Copy-and-paste the text below in your GitHub issue\n",
      "\n",
      "- `Accelerate` version: 0.22.0.dev0\n",
      "- Platform: Linux-5.4.0-99-generic-x86_64-with-glibc2.35\n",
      "- Python version: 3.10.6\n",
      "- Numpy version: 1.22.2\n",
      "- PyTorch version (GPU?): 2.0.0 (True)\n",
      "- PyTorch XPU available: False\n",
      "- PyTorch NPU available: False\n",
      "- System RAM: 2015.69 GB\n",
      "- GPU type: NVIDIA A100-SXM4-80GB\n",
      "- `Accelerate` default config:\n",
      "\t- compute_environment: LOCAL_MACHINE\n",
      "\t- distributed_type: MULTI_GPU\n",
      "\t- mixed_precision: no\n",
      "\t- use_cpu: False\n",
      "\t- debug: False\n",
      "\t- num_processes: 8\n",
      "\t- machine_rank: 0\n",
      "\t- num_machines: 1\n",
      "\t- rdzv_backend: static\n",
      "\t- same_network: False\n",
      "\t- main_training_function: main\n",
      "\t- downcast_bf16: False\n",
      "\t- tpu_use_cluster: False\n",
      "\t- tpu_use_sudo: False\n"
     ]
    }
   ],
   "source": [
    "!accelerate env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783fb1b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
