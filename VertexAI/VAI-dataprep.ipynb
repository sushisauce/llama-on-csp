{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08c13cc2-e7ba-4a69-9c98-64f7f77b0ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers  --upgrade --quiet\n",
    "!pip install datasets --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abe92dd6-5583-487a-9d55-22bc7c14f3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/jupyter/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token <token>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4578e932-b50c-4c6d-95ba-3f80011a309c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/jupyter/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-1a24287182230a5f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 15011\n",
      "{'instruction': \"What's it like to live in the belly of a whale?\", 'context': '', 'response': 'The giant whale that can swallow a human (Blue Whale) doesn\\'t have teeth, so the experience was \"painless.\" However, you would never understand how dark the darkness could be until you\\'ve been in the belly of a whale. Also, talk about being seasick. Everything inside sloshes around and smells terribly as it\\'s slowly being digested. This only adds insult to injury when it comes to feeling nauseous.', 'category': 'creative_writing'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])\n",
    "# dataset size: 15011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40463206-58b0-4417-8b19-8a5a1d676eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dolly(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e2eebb1-bcaf-442c-9359-02bd107b90f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "What channel did the series begin on?\n",
      "\n",
      "### Context\n",
      "The series began on Discovery Health Channel on November 10, 2009. Season 1 concluded on December 29, 2009, after 6 episodes. Season 2 ran from July 20, 2010, to October 19, 2010, with 8 episodes. Season 3 ran from September 1 to 29, 2011, with 6 episodes.\n",
      "\n",
      "### Answer\n",
      "The series began on Discover Health Channel on November 10, 2009.\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_dolly(dataset[randrange(len(dataset))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "928796ad-0ac1-44f7-b1e8-b1fa227f1455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-13b-hf\" # sharded weights\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,use_auth_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5a7fab4-c213-45d4-984f-8cf7a49f815a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-1a24287182230a5f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-01769950d43b8f6e.arrow\n",
      "Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-1a24287182230a5f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3b078dbcc4c7ee06.arrow\n",
      "Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-1a24287182230a5f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1b17ed058cf85374.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "Give me a list of five  healthy snacks for kids\n",
      "\n",
      "### Answer\n",
      "Peanut butter, cheese, milk, fruit and popcorn</s>\n",
      "Total number of samples: 1581\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_dolly(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "# print random sample\n",
    "print(dataset[randint(0, len(dataset))][\"text\"])\n",
    "\n",
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0f62172-afe3-4253-97af-009c7cd4153b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1581 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_dataset.save_to_disk(f\"/home/jupyter/gcs/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a08fb38-5ed0-4d9b-89bd-273163c25bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///home/jupyter/gcs/data/data-00000-of-00001.arrow [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/gcs/data/state.json [Content-Type=application/json]...\n",
      "Copying file:///home/jupyter/gcs/data/dataset_info.json [Content-Type=application/json]...\n",
      "- [3 files][ 43.3 MiB/ 43.3 MiB]                                                \n",
      "Operation completed over 3 objects/43.3 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp -r /home/jupyter/gcs/data gs://llama-bucket-syd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb3b89d9-0110-4cae-9b4b-cfb8358ee1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://llama-bucket-syd/data/\n",
      "gs://llama-bucket-syd/gcs/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://llama-bucket-syd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affdb906-63bd-461b-96ef-1cd69507827a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3306f1-6f4a-47ec-817f-0b1bb353c4f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
